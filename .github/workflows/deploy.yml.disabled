name: DigiClick AI - Deploy with Cursor Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '20.18.0'
  NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL }}
  NEXT_PUBLIC_APP_URL: ${{ secrets.NEXT_PUBLIC_APP_URL }}
  NODE_ENV: ${{ secrets.NODE_ENV }}

jobs:
  # Health Check Job
  health-check:
    name: 🔍 Health Check
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🎯 Run cursor health check
        run: npm run check:cursor

      - name: 📊 Upload health report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: health-check-report
          path: reports/

  # Lint and Format Job
  lint:
    name: 🧹 Lint & Format
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🧹 Run ESLint
        run: npm run lint

      - name: 🎨 Check Prettier formatting
        run: npm run format:check

  # Test Job
  test:
    name: 🧪 Test Suite
    runs-on: ubuntu-latest
    needs: [health-check]
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🧪 Run unit tests
        run: npm test

      - name: 🎯 Run cursor-specific tests
        run: npm run test:cursor

      - name: 📊 Upload test coverage
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: ./coverage/lcov.info
          flags: unittests
          name: codecov-umbrella

      - name: 📊 Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results
          path: |
            coverage/
            test-results/

  # Build Job
  build:
    name: 🏗️ Build Application
    runs-on: ubuntu-latest
    needs: [lint, test]
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🏗️ Build application
        run: npm run build
        env:
          NEXT_PUBLIC_API_URL: ${{ env.NEXT_PUBLIC_API_URL }}
          NEXT_PUBLIC_APP_URL: ${{ env.NEXT_PUBLIC_APP_URL }}

      - name: 📊 Analyze bundle size
        run: npm run build:analyze
        if: github.event_name == 'pull_request'

      - name: 📦 Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: build-files
          path: |
            .next/
            public/
          retention-days: 1

  # Performance Testing Job
  performance:
    name: ⚡ Performance Testing
    runs-on: ubuntu-latest
    needs: [build]
    if: github.event_name == 'pull_request'
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: build-files

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🚀 Start application
        run: npm start &
        env:
          NEXT_PUBLIC_API_URL: ${{ env.NEXT_PUBLIC_API_URL }}
          NEXT_PUBLIC_APP_URL: http://localhost:3000

      - name: ⏳ Wait for application
        run: npx wait-on http://localhost:3000 --timeout 60000

      - name: 🎯 Run Lighthouse CI
        run: npm run performance:test

      - name: 📊 Upload performance report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: lighthouse-report
          path: reports/lighthouse.json

  # Deploy to Vercel (Production)
  deploy-vercel:
    name: 🚀 Deploy to Vercel
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📥 Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: build-files

      - name: 🚀 Deploy to Vercel
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          vercel-args: '--prod'
          working-directory: ./

  # Deploy to Netlify (Production)
  deploy-netlify:
    name: 🚀 Deploy to Netlify (Production)
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: 🏗️ Build for production
        run: npm run build
        env:
          NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL }}
          NEXT_PUBLIC_APP_URL: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          NODE_ENV: ${{ secrets.NODE_ENV }}
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          GOOGLE_ANALYTICS_ID: ${{ secrets.GOOGLE_ANALYTICS_ID }}

      - name: 🔍 Verify build output
        run: |
          echo "Checking build output..."
          ls -la out/
          echo "Verifying sitemap generation..."
          ls -la public/sitemap.xml
          echo "Checking cursor system files..."
          find out/ -name "*cursor*" -type f | head -5

      - name: 🚀 Deploy to Netlify
        uses: nwtgck/actions-netlify@v3.0
        with:
          publish-dir: './out'
          production-branch: main
          github-token: ${{ secrets.GITHUB_TOKEN }}
          deploy-message: "Deploy DigiClick AI with Context-Aware Cursor System - ${{ github.event.head_commit.message }}"
          enable-pull-request-comment: true
          enable-commit-comment: true
          overwrites-pull-request-comment: true
        env:
          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}

  # Post-Deployment Testing
  post-deploy-test:
    name: 🧪 Post-Deployment Testing
    runs-on: ubuntu-latest
    needs: [deploy-netlify]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: npm ci

      - name: ⏳ Wait for deployment to be live
        run: |
          echo "Waiting for site to be available..."
          npx wait-on ${{ secrets.NEXT_PUBLIC_APP_URL }} --timeout 300000
          sleep 30

      - name: 🎯 Test cursor functionality on live site
        run: |
          echo "Testing cursor context demo page..."
          curl -f ${{ secrets.NEXT_PUBLIC_APP_URL }}/cursor-context-demo || exit 1
          echo "Testing main portfolio page..."
          curl -f ${{ secrets.NEXT_PUBLIC_APP_URL }}/portfolio || exit 1
          echo "Testing sitemap..."
          curl -f ${{ secrets.NEXT_PUBLIC_APP_URL }}/sitemap.xml || exit 1

      - name: 🔍 Verify cursor system deployment
        run: |
          echo "Checking for cursor-related assets..."
          curl -s ${{ secrets.NEXT_PUBLIC_APP_URL }} | grep -i "cursor" && echo "✅ Cursor system detected" || echo "❌ Cursor system not found"

      - name: 📊 Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: post-deploy-test-results
          path: |
            test-results/
            playwright-report/

  # Comprehensive Notification Job
  notify:
    name: 📢 Deployment Notifications
    runs-on: ubuntu-latest
    needs: [deploy-netlify, post-deploy-test, performance-audit]
    if: always()
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📊 Calculate build metrics
        id: metrics
        run: |
          # Calculate build time (approximate)
          BUILD_START_TIME="${{ github.event.head_commit.timestamp }}"
          BUILD_END_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Get commit info
          COMMIT_HASH="${{ github.sha }}"
          COMMIT_SHORT="${COMMIT_HASH:0:7}"
          COMMIT_MESSAGE="${{ github.event.head_commit.message }}"
          BRANCH_NAME="${{ github.ref_name }}"

          # Get performance audit results
          PERFORMANCE_RESULT="${{ needs.performance-audit.result }}"
          LIGHTHOUSE_SCORE="${{ needs.performance-audit.outputs.lighthouse_score || '0' }}"
          PERFORMANCE_STATUS="${{ needs.performance-audit.outputs.overall_status || 'unknown' }}"

          # Determine overall status including performance
          if [[ "${{ needs.deploy-netlify.result }}" == "success" && "${{ needs.post-deploy-test.result }}" == "success" && "$PERFORMANCE_STATUS" == "success" ]]; then
            OVERALL_STATUS="success"
            STATUS_EMOJI="✅"
            STATUS_TEXT="SUCCESS"
          elif [[ "${{ needs.deploy-netlify.result }}" == "failure" || "${{ needs.post-deploy-test.result }}" == "failure" || "$PERFORMANCE_STATUS" == "failure" ]]; then
            OVERALL_STATUS="failure"
            STATUS_EMOJI="❌"
            STATUS_TEXT="FAILURE"
          else
            OVERALL_STATUS="warning"
            STATUS_EMOJI="⚠️"
            STATUS_TEXT="WARNING"
          fi

          # Set outputs
          echo "status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "status_emoji=$STATUS_EMOJI" >> $GITHUB_OUTPUT
          echo "status_text=$STATUS_TEXT" >> $GITHUB_OUTPUT
          echo "commit_hash=$COMMIT_HASH" >> $GITHUB_OUTPUT
          echo "commit_short=$COMMIT_SHORT" >> $GITHUB_OUTPUT
          echo "commit_message=$COMMIT_MESSAGE" >> $GITHUB_OUTPUT
          echo "branch_name=$BRANCH_NAME" >> $GITHUB_OUTPUT
          echo "build_time=~5 minutes" >> $GITHUB_OUTPUT
          echo "github_run_url=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_OUTPUT
          echo "lighthouse_score=$LIGHTHOUSE_SCORE" >> $GITHUB_OUTPUT
          echo "performance_status=$PERFORMANCE_STATUS" >> $GITHUB_OUTPUT

      - name: 📧 Send email notification
        if: always()
        uses: ./.github/actions/email-notify
        with:
          smtp-server: ${{ secrets.SMTP_SERVER || 'smtp.gmail.com' }}
          smtp-port: ${{ secrets.SMTP_PORT || '587' }}
          smtp-username: ${{ secrets.SMTP_USERNAME }}
          smtp-password: ${{ secrets.SMTP_PASSWORD }}
          from-email: ${{ secrets.FROM_EMAIL }}
          to-emails: ${{ secrets.ALERT_EMAIL_RECIPIENTS }}
          status: ${{ steps.metrics.outputs.status }}
          subject: "${{ steps.metrics.outputs.status_emoji }} DigiClick AI Deployment ${{ steps.metrics.outputs.status_text }} - ${{ steps.metrics.outputs.commit_short }}"
          commit-hash: ${{ steps.metrics.outputs.commit_hash }}
          commit-message: ${{ steps.metrics.outputs.commit_message }}
          branch: ${{ steps.metrics.outputs.branch_name }}
          deployment-url: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          build-time: ${{ steps.metrics.outputs.build_time }}
          github-run-url: ${{ steps.metrics.outputs.github_run_url }}
          error-details: ${{ needs.deploy-netlify.result == 'failure' && 'Netlify deployment failed' || needs.post-deploy-test.result == 'failure' && 'Post-deployment tests failed' || '' }}

      - name: 💬 Send Slack notification
        if: always()
        uses: ./.github/actions/slack-notify
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          status: ${{ steps.metrics.outputs.status }}
          title: "DigiClick AI Deployment ${{ steps.metrics.outputs.status_text }}"
          message: ${{ steps.metrics.outputs.status == 'success' && format('Deployment completed successfully! Lighthouse Score: {0}/100, Performance: {1}', steps.metrics.outputs.lighthouse_score, steps.metrics.outputs.performance_status) || 'Deployment encountered issues. Check logs for details.' }}
          commit-hash: ${{ steps.metrics.outputs.commit_hash }}
          commit-message: ${{ steps.metrics.outputs.commit_message }}
          branch: ${{ steps.metrics.outputs.branch_name }}
          deployment-url: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          build-time: ${{ steps.metrics.outputs.build_time }}
          github-run-url: ${{ steps.metrics.outputs.github_run_url }}
          error-details: ${{ needs.deploy-netlify.result == 'failure' && 'Netlify deployment failed - check build logs' || needs.post-deploy-test.result == 'failure' && 'Post-deployment verification failed - site may be inaccessible' || '' }}

      - name: 🔍 Run comprehensive monitoring
        if: steps.metrics.outputs.status == 'success'
        run: |
          echo "🔍 Running post-deployment monitoring..."
          node scripts/deployment-monitor.js
        env:
          NEXT_PUBLIC_APP_URL: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL }}

      - name: 📊 Upload monitoring report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: deployment-monitoring-report
          path: |
            reports/
          retention-days: 30

  # Performance Budget Monitoring
  performance-audit:
    name: 🚀 Performance Budget Audit
    runs-on: ubuntu-latest
    needs: [deploy-netlify]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install dependencies
        run: |
          npm ci
          npm install -g @lhci/cli puppeteer

      - name: ⏳ Wait for deployment to be ready
        run: |
          echo "Waiting for deployment to be fully ready..."
          npx wait-on ${{ secrets.NEXT_PUBLIC_APP_URL }} --timeout 300000
          sleep 60

      - name: 🔍 Run Lighthouse CI audit
        run: |
          echo "Running Lighthouse CI performance audit..."
          lhci autorun
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
          LHCI_BUILD_CONTEXT__COMMIT_TIME: ${{ github.event.head_commit.timestamp }}

      - name: 🖱️ Run cursor system performance audit
        run: |
          echo "Running cursor system performance audit..."
          node scripts/performance-monitor.js
        env:
          NEXT_PUBLIC_APP_URL: ${{ secrets.NEXT_PUBLIC_APP_URL }}

      - name: 📊 Parse performance results
        id: performance
        run: |
          # Parse Lighthouse results
          if [ -f ".lighthouseci/assertion-results.json" ]; then
            LIGHTHOUSE_SCORE=$(cat .lighthouseci/assertion-results.json | jq -r '.[] | select(.auditProperty == "categories.performance") | .actual * 100' | head -1)
            LIGHTHOUSE_PASSED=$(cat .lighthouseci/assertion-results.json | jq -r 'map(select(.level == "error")) | length == 0')
          else
            LIGHTHOUSE_SCORE="0"
            LIGHTHOUSE_PASSED="false"
          fi

          # Parse custom performance audit
          if [ -f "reports/performance-audit-report.json" ]; then
            PERFORMANCE_STATUS=$(cat reports/performance-audit-report.json | jq -r '.overall')
            VIOLATIONS=$(cat reports/performance-audit-report.json | jq -c '.violations')
            WARNINGS=$(cat reports/performance-audit-report.json | jq -c '.warnings')
          else
            PERFORMANCE_STATUS="unknown"
            VIOLATIONS="[]"
            WARNINGS="[]"
          fi

          # Determine overall performance status
          if [ "$LIGHTHOUSE_PASSED" = "false" ] || [ "$PERFORMANCE_STATUS" = "failure" ]; then
            OVERALL_STATUS="failure"
          elif [ "$PERFORMANCE_STATUS" = "warning" ] || [ "$LIGHTHOUSE_SCORE" -lt 90 ]; then
            OVERALL_STATUS="warning"
          else
            OVERALL_STATUS="success"
          fi

          echo "lighthouse_score=$LIGHTHOUSE_SCORE" >> $GITHUB_OUTPUT
          echo "lighthouse_passed=$LIGHTHOUSE_PASSED" >> $GITHUB_OUTPUT
          echo "performance_status=$PERFORMANCE_STATUS" >> $GITHUB_OUTPUT
          echo "overall_status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "violations=$VIOLATIONS" >> $GITHUB_OUTPUT
          echo "warnings=$WARNINGS" >> $GITHUB_OUTPUT

      - name: 📊 Upload Lighthouse reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: lighthouse-reports
          path: |
            .lighthouseci/
            reports/
          retention-days: 30

      - name: 🔍 Run enhanced performance analysis
        id: enhanced_analysis
        run: |
          echo "Running enhanced performance analysis with alert manager..."

          # Create performance metrics JSON for alert manager
          cat > performance_metrics.json << EOF
          {
            "lighthouse_score": ${{ steps.performance.outputs.lighthouse_score }},
            "performance_status": "${{ steps.performance.outputs.overall_status }}",
            "violations": ${{ steps.performance.outputs.violations }},
            "warnings": ${{ steps.performance.outputs.warnings }},
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "commit_hash": "${{ github.sha }}",
            "deployment_url": "${{ secrets.NEXT_PUBLIC_APP_URL }}",
            "github_run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          }
          EOF

          # Run performance alert manager
          node scripts/performance-alert-manager.js performance_metrics.json || echo "Alert manager completed with warnings"

          # Extract enhanced metrics for notifications
          ALERT_LEVEL=$(jq -r '.alert_level // "unknown"' performance_metrics.json 2>/dev/null || echo "unknown")
          CRITICAL_COUNT=$(echo '${{ steps.performance.outputs.violations }}' | jq '[.[] | select(.severity == "critical")] | length' 2>/dev/null || echo "0")
          WARNING_COUNT=$(echo '${{ steps.performance.outputs.violations }}' | jq '[.[] | select(.severity == "warning")] | length' 2>/dev/null || echo "0")

          echo "alert_level=$ALERT_LEVEL" >> $GITHUB_OUTPUT
          echo "critical_count=$CRITICAL_COUNT" >> $GITHUB_OUTPUT
          echo "warning_count=$WARNING_COUNT" >> $GITHUB_OUTPUT

      - name: 🚨 Send enhanced performance alerts
        if: always()
        uses: ./.github/actions/performance-notify
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          email-config: ${{ secrets.EMAIL_CONFIG }}
          performance-status: ${{ steps.performance.outputs.overall_status }}
          lighthouse-score: ${{ steps.performance.outputs.lighthouse_score }}
          violations: ${{ steps.performance.outputs.violations }}
          warnings: ${{ steps.performance.outputs.warnings }}
          lighthouse-report-url: ${{ steps.performance.outputs.lighthouse_report_url }}
          pagespeed-url: "https://pagespeed.web.dev/report?url=${{ secrets.NEXT_PUBLIC_APP_URL }}"
          deployment-url: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          commit-hash: ${{ github.sha }}
          github-run-url: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

      - name: 🚨 Send failure alert with details
        if: steps.metrics.outputs.status == 'failure'
        uses: ./.github/actions/slack-notify
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          status: failure
          title: "🚨 URGENT: DigiClick AI Deployment Failed"
          message: |
            Immediate action required! Deployment has failed and may impact live site.

            **Next Steps:**
            1. Check GitHub Actions logs
            2. Verify Netlify deployment status
            3. Test cursor system functionality
            4. Consider rollback if necessary
          commit-hash: ${{ steps.metrics.outputs.commit_hash }}
          commit-message: ${{ steps.metrics.outputs.commit_message }}
          branch: ${{ steps.metrics.outputs.branch_name }}
          github-run-url: ${{ steps.metrics.outputs.github_run_url }}
          error-details: "Build: ${{ needs.build.result }} | Deploy: ${{ needs.deploy-netlify.result }} | Tests: ${{ needs.post-deploy-test.result }}"

  # Comprehensive Deployment Verification
  deployment-verification:
    name: 🔍 Comprehensive Deployment Verification
    runs-on: ubuntu-latest
    needs: [deploy-netlify, performance-audit]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔍 Run comprehensive deployment verification
        id: verification
        uses: ./.github/actions/deployment-verify
        with:
          deployment-url: ${{ secrets.NEXT_PUBLIC_APP_URL }}
          backend-url: ${{ secrets.NEXT_PUBLIC_API_URL }}
          slack-webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          verification-timeout: '600'
          enable-performance-tests: 'true'
          enable-ab-testing-verification: 'true'

      - name: 📊 Analyze verification results
        id: analysis
        run: |
          VERIFICATION_STATUS="${{ steps.verification.outputs.verification-status }}"
          PAGES_SUCCESS_RATE=$(echo "scale=2; ${{ steps.verification.outputs.pages-successful }} * 100 / ${{ steps.verification.outputs.pages-tested }}" | bc -l 2>/dev/null || echo "0")

          echo "verification_status=$VERIFICATION_STATUS" >> $GITHUB_OUTPUT
          echo "pages_success_rate=$PAGES_SUCCESS_RATE" >> $GITHUB_OUTPUT

          # Determine if deployment should be considered successful
          if [ "$VERIFICATION_STATUS" = "success" ]; then
            echo "deployment_success=true" >> $GITHUB_OUTPUT
          elif [ "$VERIFICATION_STATUS" = "warning" ] && [ "$(echo "$PAGES_SUCCESS_RATE >= 90" | bc -l)" = "1" ]; then
            echo "deployment_success=true" >> $GITHUB_OUTPUT
          else
            echo "deployment_success=false" >> $GITHUB_OUTPUT
          fi

          echo "📊 Verification Analysis:"
          echo "Status: $VERIFICATION_STATUS"
          echo "Page Success Rate: $PAGES_SUCCESS_RATE%"
          echo "Cursor Variants Working: ${{ steps.verification.outputs.cursor-variants-working }}"
          echo "A/B Testing: ${{ steps.verification.outputs.ab-testing-status }}"
          echo "Backend: ${{ steps.verification.outputs.backend-status }}"

      - name: 🚨 Handle critical verification failures
        if: steps.verification.outputs.verification-status == 'critical_failure'
        run: |
          echo "🚨 CRITICAL FAILURE: Deployment verification failed critically"
          echo "This indicates major issues that require immediate attention"
          echo "Consider emergency rollback procedures"

          # Set job status to failure for critical issues
          exit 1

      - name: ⚠️ Handle verification warnings
        if: steps.verification.outputs.verification-status == 'warning'
        run: |
          echo "⚠️ WARNING: Deployment verification completed with warnings"
          echo "Monitor the deployment closely and address issues promptly"
          echo "Pages tested: ${{ steps.verification.outputs.pages-successful }}/${{ steps.verification.outputs.pages-tested }}"
          echo "Cursor variants: ${{ steps.verification.outputs.cursor-variants-working }}"

      - name: ✅ Confirm successful verification
        if: steps.verification.outputs.verification-status == 'success'
        run: |
          echo "✅ SUCCESS: Deployment verification completed successfully"
          echo "All systems are functioning correctly"
          echo "Pages: ${{ steps.verification.outputs.pages-successful }}/${{ steps.verification.outputs.pages-tested }}"
          echo "Cursor system: ${{ steps.verification.outputs.cursor-variants-working }} variants working"
          echo "A/B testing: ${{ steps.verification.outputs.ab-testing-status }}"
          echo "Backend: ${{ steps.verification.outputs.backend-status }}"
